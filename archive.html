<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Past Seminars - Digital Brain Seminar</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Noto+Sans+JP:wght@400;500;700&display=swap"
        rel="stylesheet">

    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="archive.css">
    <link rel="icon" type="image/png" href="brain-logo.png">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
</head>

<body>

    <nav class="navbar">
        <a href="index.html" class="nav-logo"><img src="brain-logo.png" alt="Logo" class="logo-icon"> Digital Brain
            Seminar</a>

        <div class="hamburger" id="hamburger-btn">
            <span></span>
            <span></span>
            <span></span>
        </div>

        <div class="nav-links" id="nav-links">
            <a href="index.html">Upcoming</a>
            <a href="archive.html" class="active">Past Seminars</a>
            <a href="https://www.youtube.com/@kennakae2779/videos" target="_blank">YouTube</a>
            <a href="https://docs.google.com/forms/d/1duZBmrP8-1nVFSevK-VwitDGuL3oonJRR4SJCReSnrM/viewform?edit_requested=true"
                target="_blank">Register</a>
        </div>
    </nav>

    <section class="archive-hero">
        <canvas id="hero-canvas"></canvas>
        <div class="container">
            <h1>Past Seminars</h1>
            <p>Archive of past seminars. Click on a card to view details.</p>
        </div>
    </section>

    <div class="container">
        <!-- Year Navigation -->
        <div class="year-nav">
            <a href="#year-2026" class="year-tab active">2026</a>
            <a href="#year-2025" class="year-tab">2025</a>
            <a href="#year-2024" class="year-tab">2024</a>
        </div>

        <!-- 2026 Seminars -->
        <section id="year-2026" class="year-section">
            <h2>2026</h2>

            <!-- 2026-1 Yamazaki -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2026/1/13 Tue 13:00-14:30 (JST)</div>
                    <h3 class="card-title">Development of a biophysically detailed neural circuit simulator and its application to whole mouse cortex simulations</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://researchmap.jp/tyam" target="_blank"
                            rel="noopener noreferrer" style="text-decoration: underline;">Tadashi Yamazaki</a></div>
                        <div class="speaker-affiliation">The University of Electro-Communications</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> Understanding the brain requires linking microscopic biophysical
                        properties (e.g., ion channel dynamics and dendritic morphology) to emergent macroscopic
                        phenomena (e.g., neural oscillations and network dynamics). While biologically detailed models
                        are instrumental for this mechanistic insight, their inherent computational cost is substantial.
                        While conventional simulators like NEURON and Arbor are highly effective for general use, in
                        simulating models at the extreme scale, they could become inefficient. This is primarily in
                        terms of their scalability and optimization to fully leverage modern High-Performance Computing
                        (HPC) architectures.</p>

                    <p>We introduce Neulite (<a href="https://numericalbrain.org/neulite/" target="_blank"
                            rel="noopener noreferrer">https://numericalbrain.org/neulite/</a>), a light-weight
                        biophysically detailed neural circuit simulator. It is architecturally defined by two core
                        components: a frontend compliant with the Allen Institute‚Äôs Brain Modeling ToolKit (BMTK), and a
                        portable numerical kernel. The frontend facilitates biological plausibility and reproducibility
                        through the utilization of standardized data. The kernel, which can be specifically tuned for
                        different computing architectures, allows us to overcome the limitations of conventional
                        simulators through optimized, domain-specific algorithms.</p>

                    <p>Neulite has been utilized to successfully execute the Allen Institute‚Äôs whole mouse cortex model
                        on the Supercomputer Fugaku. We used the entire Fugaku system to simulate 9 million biologically
                        detailed neurons and 26 billion synapses, demonstrating a significant scale of computation. This
                        work is the result of an international collaboration with the team of Dr. Anton Arkhipov at the
                        Allen Institute, where their comprehensive model met our high-performance simulation technology,
                        with support from key domestic contributors (RIKEN R-CCS, RIST, and Yamaguchi University).
                        Neulite is therefore a valuable tool for achieving data-driven, large-scale modeling and
                        advancing the understanding of how cellular properties influence overall neural circuit
                        function.</p>

                    <p>The content of this talk has been published in a conference paper [1].</p>

                    <p style="font-size: 0.9em; margin-top: 1em;">[1] Rin Kuriyama, Kaaya Akira, Laura Green, Beatriz
                        Herrera, Kael Dai, Mari Iura, Gilles Gouaillardet, Asako Terasawa, Taira Kobayashi, Jun
                        Igarashi, Anton Arkhipov, Tadashi Yamazaki (*: equally contributed). Microscopic-Level Mouse
                        Whole Cortex Simulation Composed of 9 Million Biophysical Neurons and 26 Billion Synapses on the
                        Supercomputer Fugaku. in The International Conference for High Performance Computing,
                        Networking, Storage and Analysis (SC ‚Äô25), November 16‚Äì21, 2025, St Louis, MO, USA. ACM, New
                        York, NY, USA, 11 pages. doi: 10.1145/3712285.3759819.</p>
                </div>
            </article>
        </section>

        <!-- 2025 Seminars -->
        <section id="year-2025" class="year-section">
            <h2>2025</h2>

            <!-- 2025 - Honda Naoki -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/12/10 Wed 14:30-16:30 (JST)</div>
                    <h3 class="card-title">Revisiting Sperry in the Age of Connectomics: Genetic Rules of Brain-Wide
                        Wiring</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://sites.google.com/view/data-driven-biology/"
                                target="_blank" rel="noopener noreferrer" style="text-decoration: underline;">Naoki
                                Honda</a></div>
                        <div class="speaker-affiliation">Nagoya
                            University</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> Understanding how brain-wide neural circuits are
                        genetically organized remains one of the fundamental challenges in neuroscience. Roger Sperry's
                        classical chemoaffinity theory proposed that molecular gradients provide positional cues for
                        axonal wiring, yet its application has been largely limited to localized sensory systems. Here,
                        we introduce SPERRFY (Spatial Positional Encoding for Reconstructing Rules of axonal Fiber
                        connectivitY), a data-driven framework to examine whether Sperry's concept can be generalized to
                        the entire brain. By integrating mesoscale connectomic data with spatial transcriptomic maps
                        from the Allen Mouse Brain Atlas, SPERRFY applies canonical correlation analysis (CCA) to
                        identify latent correlated structures between gene-expression and connectivity spaces, thereby
                        inferring positional gradients that may reflect molecular constraints underlying long-range
                        axonal organization. This framework bridges molecular and anatomical levels of organization,
                        providing a quantitative basis for reinterpreting Sperry's chemoaffinity theory in the context
                        of whole-brain connectomics.</p>
                </div>
            </article>

            <!-- 2025-1 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/10/7 Tue 13:00-14:30 (JST)</div>
                    <h3 class="card-title">Behavioural and neural mechanism of social metacognition in predicting
                        others' performance</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://www.kentaromiyamoto-lab.com/" target="_blank"
                                rel="noopener noreferrer" style="text-decoration: underline;">Kentaro Miyamoto</a></div>
                        <div class="speaker-affiliation">RIKEN
                            CBS</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> When we collaborate with others to tackle new
                        challenges, we anticipate the roles that each team member will play, consider our own role based
                        on these predictions, and adjust our behaviour accordingly. Because each member differs in
                        experience and skills, it is necessary to flexibly adapt the strategy used for prediction. For
                        example, when interacting with beginners who have less career experience, we can make reasonable
                        predictions about their performance by projecting adjusting introspection ('social
                        metacognition'). In contrast, the same strategy cannot be applied to experts with more extensive
                        experience. In this talk, I will introduce our latest research addressing this issue.
                        Specifically, we found that the anterior lateral prefrontal cortex (area 47) is engaged when
                        predicting the thoughts of beginners through social metacognition, whereas the temporoparietal
                        junction (TPJ) is recruited when predicting the thoughts of experts based on heuristics. The
                        lecture will begin with an overview of the foundations of metacognition and then present the
                        studies that led to these findings.</p>
                    <div class="card-links">
                        <a href="https://www.youtube.com/watch?v=L0uATpNPuT0" target="_blank" class="link-btn">üé•
                            YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2025-2 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/8/27 Wed 15:00-16:30 (JST)</div>
                    <h3 class="card-title">Dissecting the contribution to perceptual decisions of encoding and readout
                        of neural information</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a
                                href="https://www.uke.de/english/departments-institutes/institutes/department-of-excellence-for-neural-information-processing/team/index.html"
                                target="_blank" rel="noopener noreferrer" style="text-decoration: underline;">Stefano
                                Panzeri</a></div>
                        <div class="speaker-affiliation">University Medical Center Hamburg-Eppendorf</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> Perceptual decisions require that neural populations
                        encode information about the sensory environment and that other downstream populations read it
                        out to inform behavioral outputs. Here we present our computational work to provide methods that
                        individuate and tease apart the contribution of these two neural operations to the formation of
                        perceptual decisions. We exemplify these methods with the study of several datasets from sensory
                        and parietal cortices and we discuss their implications for understanding the emergent
                        computations of neural population codes.</p>
                    <div class="card-links">
                        <a href="https://youtu.be/UyTtfP5wrW8" target="_blank" class="link-btn">üé•
                            YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2025-3 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/6/13 Fri 10:30-12:00 (JST)</div>
                    <h3 class="card-title">Integrating multimodal data for bio-realistic simulations of brain circuits
                    </h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://alleninstitute.org/person/anton-arkhipov/"
                                target="_blank" rel="noopener noreferrer" style="text-decoration: underline;">Anton
                                Arkhipov</a></div>
                        <div class="speaker-affiliation">Allen
                            Institute</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> A central question in neuroscience is how the
                        structure of the brain determines its activity and function. To explore this systematically, we
                        develop large-scale bio-realistic simulations of brain circuits, which integrate a broad array
                        of experimental data: Distribution and morpho-electric properties of different neuron types;
                        Connection probabilities, synaptic weights, axonal delays, and dendritic targeting rules; And a
                        representation of inputs into the simulated circuits from other parts of the brain. We will
                        discuss this approach focusing on the 230,000-neuron model of mouse primary visual cortex (area
                        V1). Simulations of neural activity in the model match experimental recordings in vivo on a
                        number of metrics, such as firing rates, direction selectivity, and others. Applications include
                        the following problems of broad interest: Understanding how architecture of brain circuit gives
                        rise to the observed functional activity; Learning of behavioral and computational tasks in
                        biological and artificial networks; Generation of the extracellular electric potential due to
                        synaptic activity in the cortex. The model is shared freely with the community via
                        brain-map.org, as are the datasets it is based on.</p>
                    <div class="card-links">
                        <a href="https://www.nature.com/articles/s41593-025-01904-7.pdf" target="_blank"
                            class="link-btn">üìÑ Nature Neuroscience Paper</a>
                        <a href="https://youtu.be/CtPOaAII5Ng" target="_blank" class="link-btn">üé• YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2025-4 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/3/27 Thu 17:30-19:00 (JST)</div>
                    <h3 class="card-title">A computational approach to evaluate how molecular mechanisms impact
                        large-scale brain activity</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a
                                href="https://neuropsi.cnrs.fr/en/departments/icn/group-leader-alain-destexhe/"
                                target="_blank" rel="noopener noreferrer" style="text-decoration: underline;">Alain
                                Destexhe</a></div>
                        <div class="speaker-affiliation">NeuroPSI</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> This seminar will present the authors' recent
                        preprint on a computational approach to evaluate how molecular mechanisms impact large-scale
                        brain activity.</p>
                    <div class="card-links">
                        <a href="https://doi.org/10.21203/rs.3.rs-4610184/v1" target="_blank" class="link-btn">üìÑ
                            Preprint</a>
                    </div>
                </div>
            </article>

            <!-- 2025-5 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/2/25 Tue 17:00-18:30 (JST)</div>
                    <h3 class="card-title">EBRAINS Seminar D: High Performance Computing and Co-Design</h3>
                    <div class="speaker">
                        <div class="speaker-entry">
                            <div class="speaker-name">Jan Bjaalie</div>
                        </div>
                        <div class="speaker-entry">
                            <div class="speaker-name">Ekaterina Zossimova</div>
                            <div class="speaker-affiliation">Co-Design & Science Support</div>
                        </div>
                        <div class="speaker-entry">
                            <div class="speaker-name">Lena Oden</div>
                            <div class="speaker-affiliation">EBRAINS Base infrastructure and HPC services</div>
                        </div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Topics:</strong> Co-Design & Science Support, EBRAINS Base
                        infrastructure and HPC services</p>
                    <div class="card-links">
                        <a href="https://digitalbrainproject.github.io/seminar/20250225_EBRAINS_D.html" target="_blank"
                            class="link-btn">üîó Details</a>
                    </div>
                </div>
            </article>

            <!-- 2025-6 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/2/18 Tue 17:00-18:30 (JST)</div>
                    <h3 class="card-title">EBRAINS Seminar C: Research Infrastructure and Education</h3>
                    <div class="speaker">
                        <div class="speaker-entry">
                            <div class="speaker-name">Wouter Klijn</div>
                            <div class="speaker-affiliation">EBRAINS-RI architecture</div>
                        </div>
                        <div class="speaker-entry">
                            <div class="speaker-name">Franziska Vogel</div>
                            <div class="speaker-affiliation">EBRAINS Education</div>
                        </div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Topics:</strong> EBRAINS-RI architecture, multiple scales of complexity,
                        EBRAINS Education and events for early career researcher</p>
                    <div class="card-links">
                        <a href="https://digitalbrainproject.github.io/seminar/20250218_EBRAINS_C.html" target="_blank"
                            class="link-btn">üîó Details</a>
                    </div>
                </div>
            </article>

            <!-- 2025-7 (9th Seminar) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/2/17 Mon 10:30„Äú (JST)</div>
                    <h3 class="card-title">9th Seminar: High-dimensional interpretable factor analysis via penalization
                    </h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://keihirose.com/" target="_blank"
                                rel="noopener noreferrer" style="text-decoration: underline;">Kei Hirose</a></div>
                        <div class="speaker-affiliation">Kyushu University</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Place:</strong> Zoom (Please find the zoom link by the registration)</p>
                    <p class="abstract"><strong>Abstract:</strong> Factor analysis is a statistical method for
                        identifying latent factors from the correlation structures of high-dimensional data. It was
                        originally developed for applications in social and behavioral sciences but has since been
                        applied to various research fields, including the natural sciences. An advantage of factor
                        analysis is that it leads to interpretable latent factors, enabling applications such as the
                        identification of active brain regions in neuroscience. In this study, we propose a penalized
                        maximum likelihood estimation method aimed at enhancing the interpretability of latent factors.
                        In particular, the Prenet (Product-based elastic net) penalization allows for the estimation of
                        a perfect simple structure, a desirable characteristic in the factor analysis literature. The
                        usefulness of the proposed method is investigated through real data analyses. Finally, we
                        discuss potential extensions and applications of the proposed method in neuroscience.</p>
                </div>
            </article>

            <!-- 2025-8 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/1/30 Thu 9:00-16:00 (JST)</div>
                    <h3 class="card-title">Allen Neural Dynamics Workshop at OIST</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://alleninstitute.org/person/jim-berg/" target="_blank"
                                rel="noopener noreferrer" style="text-decoration: underline;">James Berg</a>, <a
                                href="https://alleninstitute.org/person/saskia-de-vries/" target="_blank"
                                rel="noopener noreferrer" style="text-decoration: underline;">Saskia de Vries</a></div>
                        <div class="speaker-affiliation">Allen
                            Institute for Neural Dynamics</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Location:</strong> Okinawa Institute of Science and Technology (on-site
                        only)</p>
                    <div class="card-links">
                        <a href="https://www.oist.jp/conference/allen-neural-dynamics-workshop" target="_blank"
                            class="link-btn">üîó Workshop Page</a>
                        <a href="https://youtu.be/KwIbGYFokyM" target="_blank" class="link-btn">üé• Movie 1</a>
                        <a href="https://youtu.be/Qh1y_ix8gXE" target="_blank" class="link-btn">üé• Movie 2</a>
                    </div>
                </div>
            </article>

            <!-- 2025-9 (Collaborative Seminar with EBRAINS) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/1/28 Tue 17:00- (JST)</div>
                    <h3 class="card-title">Collaborative Seminar with EBRAINS</h3>
                    <div class="speaker">
                        <div class="speaker-name">Susanne Kunkel, Thorsten Hater, and others</div>
                        <div class="speaker-affiliation">EBRAINS Research Infrastructure</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <div class="talk-section" style="margin-bottom: 2rem;">
                        <p><strong>Data & Contents 1/28 17:00- (JST)</strong></p>
                        <ul
                            style="list-style: none; padding: 0; margin: 0 0 1.5rem 0; line-height: 1.6; font-size: 0.95rem; color: #1e293b;">
                            <li>17:00 - 17:30 (Susanne Kunkel) The NEST ecosystem: A key enabler of efficient
                                brain-scale spiking network simulation and sustainable neuroscience research</li>
                            <li>17:30 - 18:00 (Thorsten Hater) Multiscale Simulations of Full Brain Models using Arbor
                                and TVB</li>
                            <li>18:00 - 18:30 (Various) Flashlight talks of developing integrated EBRAINS-RI workflows:
                            </li>
                            <li style="padding-left: 1rem;">¬∑ Sharon Yates &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; QUINT workflow
                            </li>
                            <li style="padding-left: 1rem;">¬∑ Giulia De Bonis &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; CobraWAP
                                Workflow</li>
                            <li style="padding-left: 1rem;">¬∑ Wouter Klijn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Virtual Brain
                                Twin workflow</li>
                        </ul>
                    </div>

                    <div class="talk-section"
                        style="margin-bottom: 2rem; border-top: 1px solid var(--border); padding-top: 1.5rem;">
                        <h4 style="margin-bottom: 1rem; color: var(--accent);">The NEST ecosystem: A key enabler of
                            efficient brain-scale spiking network simulation and sustainable neuroscience research</h4>
                        <p class="abstract" style="margin-bottom: 1rem;">NEST [2] is a powerful simulation engine that
                            has evolved with the neuroscience community over a quarter-century. The simulator has been
                            continuously advanced and extended to tackle new scientific questions and to push the
                            boundaries of large-scale brain simulation at the resolution of single neurons and synapses.
                            The code is extremely scalable, where recent technological advancements also target
                            GPU-based systems [3]. The graphical frontend NEST Desktop [4] and the modelling language
                            NESTML [5] make the NEST ecosystem easily accessible to new users.</p>
                        <p class="abstract" style="margin-bottom: 1rem;">The NEST ecosystem is open source and developed
                            under the umbrella of the NEST Initiative [1]. This community organization promotes the use
                            of standard high-quality tools in neuroscience and related fields such as artificial
                            intelligence and neurorobotics, thus fostering sustainable research. In the EBRAINS research
                            infrastructure, NEST provides key functionality for research into the dynamics, structure,
                            and function of brain-scale spiking networks [6].</p>
                        <div style="font-size: 0.85em; color: #64748b; margin-top: 0.5rem; line-height: 1.5;">
                            1. <a href="https://www.nest-initiative.org" target="_blank"
                                style="text-decoration: underline;">https://www.nest-initiative.org</a><br>
                            2. <a href="https://nest-simulator.readthedocs.io" target="_blank"
                                style="text-decoration: underline;">https://nest-simulator.readthedocs.io</a><br>
                            3. <a href="https://nest-gpu.readthedocs.io" target="_blank"
                                style="text-decoration: underline;">https://nest-gpu.readthedocs.io</a><br>
                            4. <a href="https://nest-desktop.readthedocs.io" target="_blank"
                                style="text-decoration: underline;">https://nest-desktop.readthedocs.io</a><br>
                            5. <a href="https://nestml.readthedocs.io" target="_blank"
                                style="text-decoration: underline;">https://nestml.readthedocs.io</a><br>
                            6. <a href="https://www.ebrains.eu/tools/nest" target="_blank"
                                style="text-decoration: underline;">https://www.ebrains.eu/tools/nest</a>
                        </div>
                    </div>

                    <div class="talk-section" style="margin-bottom: 2rem;">
                        <h4 style="margin-bottom: 1rem; color: var(--accent);">Multiscale Simulations of Full Brain
                            Models using Arbor and TVB</h4>
                        <div style="font-size: 0.9em; margin-bottom: 0.5rem; color: #64748b; line-height: 1.4;">
                            J. Courson, T. Manos (ETIS Lab, ENSEA, CNRS, UMR8051, CY Cergy-Paris University)<br>
                            S. Diaz, T. Hater, H. Lu, M. v.d.Vlag (Forschungszentrum J√ºlich)
                        </div>
                        <p class="abstract"><strong>Abstract:</strong> Simulating full-brain dynamics at neuron
                            resolution is a challenge still far out of the reach of computational neuroscience. Neural
                            mass models, as implemented in The Virtual Brain (TVB) [^tvb], capture whole brain dynamics
                            with a coarse or fine grid at the brain area level and have made personalized medicine and
                            clinical interventions feasible [^tvb-epilepsy]. At the other end of the spectrum,
                            computational tools like Arbor [^arb] enable us to model neurons with full morphological
                            details but with only fractions of the required counts of neurons and synapses. In this
                            study, we present an attempt to combine TVB and Arbor in a practical approach ---
                            simultaneously simulating a given brain region at the microscopic level and neural mass
                            models to provide a salient approximation of the neural activity of the remaining brain
                            areas. We build a scalable co-simulation workflow by leveraging the interfaces provided by
                            both Arbor and TVB to communicate with other external simulators. The resulting framework is
                            flexible as it employs existing models available in both simulators respectively with only
                            minor changes. Additionally, synaptic connections bridging models between TVB and Arbor are
                            specified. This feature allows for swapping dynamics on a per-region (TVB) or per-cell
                            (Arbor) basis without redefining the workflow. Thus, different parts of the brain can be
                            modeled and simulated in detail without sweeping changes to the overall model. We show here
                            some preliminary applications of this Arbor-TVB framework and workflow to simulate the
                            dynamics of seizure propagation.</p>
                        <div
                            style="font-size: 0.85em; color: #64748b; margin-top: 1rem; border-top: 1px solid #e2e8f0; padding-top: 0.5rem; line-height: 1.5;">
                            1. A Morphologically-Detailed Neural Network Simulation Library for Contemporary
                            High-Performance Computing Architectures (Abi Akar, N. et al, 2019)<br>
                            2. The Virtual Epileptic Patient: Individualized whole-brain models of epilepsy spread (V.K.
                            Jirsa et al, 2017)<br>
                            3. The Virtual Brain: a simulator of primate brain network dynamics (Sanz Leon, P. et al,
                            2013)<br>
                            4. A unified physiological framework of transitions between seizures, sustained ictal
                            activity and depolarization block at the single neuron level (Depannemaecker, D. et al;
                            2022)
                        </div>
                    </div>


                </div>
            </article>

            <!-- 2025-10 (3rd Digital Brain Tutorial) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/1/16 Thu 13:00„Äú (JST)</div>
                    <h3 class="card-title">3rd Digital Brain Tutorial</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://hyoka.ofc.kyushu-u.ac.jp/html/100023049_ja.html"
                                target="_blank" rel="noopener noreferrer" style="text-decoration: underline;">Áî∞‰∏ä Â§ßÂä©</a>
                        </div>
                        <div class="speaker-affiliation">‰πùÂ∑ûÂ§ßÂ≠¶</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract" style="margin-bottom: 0.5rem;"><strong>Place:</strong> Zoom (Please find the
                        zoom link by the registration)</p>
                    <p class="abstract"><strong>Abstract:</strong>
                        „Çè„ÅåÂõΩ„ÅØ‚ªëÂπ¥„Å´Ê∏°„ÇäÊï∞ÁêÜÁßëÂ≠¶ÂàÜÈáé„ÅßÈ´ò„ÅÑÊ∞¥Ê∫ñ„ÇíÁ∂≠ÊåÅ„Åó„Å¶„Åç„ÅüÔºé„Åó„Åã„ÅóÁèæÂú®„ÅÆÁ§æ‰ºö„Å´„Åä„Åë„ÇãDXÂåñ„ÅÆÊµÅ„Çå„ÅÆ‰∏≠„ÅßÔºåÁî£Ê•≠Áïå„ÉªË´∏ÁßëÂ≠¶ÂàÜÈáé„ÉªÁ§æ‰ºö„Åã„Çâ„ÅÆË™≤È°å„Å´Âøú„Åà‰æ°ÂÄ§„ÇíÂÖ±Ââµ„Åô„ÇãÊñ∞„Åü„Å™Êï∞ÁêÜÈÄ£Êê∫Âü∫Áõ§„ÇíÁØâ„ÅèÂøÖË¶Å„Å´Ëø´„Çâ„Çå„Å¶„ÅÑ„ÇãÔºé„Åì„ÅÆ„Çà„ÅÜ„Å™Ë¶ÅË´ã„Å´Êï∞Â≠¶„Ç≥„Éü„É•„Éã„ÉÜ„Ç£ÂÖ®‰Ωì„ÅßÂøú„ÅàÔºåÁ∑èÂêàÁü•ÊßãÁØâ„ÇíÂÆüÁèæ„Åô„Çã„Ç™„Éº„É´„Ç∏„É£„Éë„É≥‰ΩìÂà∂„ÅÆ„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„ÅÆÊßãÁØâ„ÇíÁõÆÊåá„Åó„Å¶Ôºå2023Âπ¥10Êúà„Å´ÂÖ®ÂõΩ17„ÅÆÊï∞Â≠¶„ÉªÊï∞ÁêÜÁßëÂ≠¶„Å´Èñ¢ÈÄ£„Åô„ÇãÊ©üÈñ¢„ÅåÂèÇÁîª„ÅóÔºå„Éû„Çπ„Éª„Éï„Ç©„Ç¢„Éª„Ç§„É≥„ÉÄ„Çπ„Éà„É™„Éª„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†
                        (MfIP) „ÇíÁô∫Ë∂≥„Åó„ÅüÔºéMfIP„ÅÆÊ¥ªÂãï„ÅÆ‰∏Ä„Å§„Å®„Åó„Å¶ÔºåËÑ≥ÁßëÂ≠¶ÂàÜÈáé„ÅÆÁöÜÊßò„Å®ÔºåÈÄ£Êê∫Êé¢Á¥¢„ÉØ„Éº„ÇØ„Ç∑„Éß„ÉÉ„Éó„ÅÆÈñãÂÇ¨ (2023Âπ¥12Êúà28Êó•@Êó•Êú¨Ê©ã„É©„Ç§„Éï„Çµ„Ç§„Ç®„É≥„Çπ„Éì„É´„Éá„Ç£„É≥„Ç∞)ÔºåDigital
                        Brain Seminar/Workshop„ÅÆÈñãÂÇ¨ (2024/9/19„Äú21Êó•@Êó•Êú¨Ê©ã„É©„Ç§„Éï„Çµ„Ç§„Ç®„É≥„Çπ„Éì„É´„Éá„Ç£„É≥„Ç∞)
                        „Å™„Å©„ÅßÈÄ£Êê∫„ÇíÈÄ≤„ÇÅ„Å¶„Åç„ÅüÔºéÊú¨„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´„Åß„ÅØÔºå„Åï„Çâ„Å™„ÇãÈÄ£Êê∫„ÅÆÊ∑±Âåñ„ÇíÁõÆÊåá„Åó„Å¶ÔºåMfIPÏùò Ê¥ªÂãï„ÇÑÈÄ£Êê∫Ê©üÈñ¢„Å´ÊâÄÂ±û„Åô„ÇãÊï∞Â≠¶ËÄÖ„ÅÆÊåÅ„Å§„Ç∑„Éº„Ç∫„Å´„Å§„ÅÑ„Å¶Á¥π‰ªã„Åô„ÇãÔºé</p>
                    <div class="card-links">
                        <a href="https://www.youtube.com/watch?v=nZx3RaYovHE" target="_blank" class="link-btn">üé•
                            YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2025-11 (Collaborative Seminar with Allen Institute) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/1/16 Thu 9:00„Äú (JST)</div>
                    <h3 class="card-title">Collaborative Seminar with Allen Institute: Science at the Allen Institute
                        for Neural Dynamics (AIND)</h3>
                    <div class="speaker">
                        <div class="speaker-name">Karel Svoboda</div>
                        <div class="speaker-affiliation">Allen Institute for Neural Dynamics</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract" style="margin-bottom: 0.5rem;"><strong>Place:</strong> Zoom (Please find the
                        zoom link by the registration)</p>
                    <p class="abstract"><strong>Abstract:</strong> Our goal is to discover how the brain produces out
                        emotions, memories, and actions. Answers will be in terms of neural activity in defined neuron
                        types interacting across the whole brain and body. To advance our goals we are developing
                        next-generation neurotechnologies. We are also committed to Open Science: Knowledge, data, and
                        tools will be widely shared, to facilitate science elsewhere and to support the development of
                        therapies for brain disorders. I will describe how AIND scientists and engineers organize for
                        team science. I will then discuss a few recent neurotechnological and scientific advances.</p>
                </div>
            </article>

            <!-- 2025-12 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2025/1/7 Tue 15:00-16:30 (JST)</div>
                    <h3 class="card-title">Emergence of Cognitive Functions in Natural and Artificial Neural Networks
                    </h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://cogi.kaist.ac.kr/" target="_blank"
                                rel="noopener noreferrer" style="text-decoration: underline;">Se-Bum Paik</a></div>
                        <div class="speaker-affiliation">KAIST</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> How do the diverse functions of the brain originate?
                        Understanding the developmental mechanisms that underlie brain functions is a fundamental
                        question in neuroscience, with significant implications for research on artificial neural
                        networks. This talk will introduce principles related to these developmental mechanisms, which
                        differ notably from the data-driven learning paradigms predominantly used in AI. I will present
                        our recent findings demonstrating that early functional circuits and cognitive functions in the
                        brain can emerge spontaneously, even in the complete absence of training. Using a biologically
                        inspired neural network model, I will first show how regularly structured functional maps can
                        arise from simple local interactions between individual cells. I will discuss how evolutionary
                        variations in physical parameters may lead to the development of distinct functional circuitry
                        in the brain. Next, I will demonstrate that higher cognitive functions, such as visual quantity
                        estimation and primitive object detection, can also emerge spontaneously in untrained neural
                        networks. I will argue that random feedforward connections in early circuits may be sufficient
                        to initiate functional circuits. These findings suggest that early visual functions can emerge
                        from the statistical properties of bottom-up projections in hierarchical neural networks,
                        providing insight into the origins of primitive functions in the brain.</p>
                </div>
            </article>
        </section>

        <!-- 2024 Seminars -->
        <section id="year-2024" class="year-section">
            <h2>2024</h2>

            <!-- 2024-1 (EBRAINS-RI Seminar) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/12/3 Tue 17:00-18:30 JST, 9:00 - 10:30 CET</div>
                    <h3 class="card-title">Introduction to the EBRAINS-RI ‚Äì An overview of the Infrastructure, Data
                        sharing and Brain Atlas</h3>
                    <div class="speaker">
                        <div class="speaker-name">Katrin Amunts, Trygve Leergaard, Oliver Schmid</div>
                        <div class="speaker-affiliation">EBRAINS Research Infrastructure</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <div class="schedule-section" style="margin-bottom: 1.5rem;">
                        <p style="margin-bottom: 0.5rem;"><strong>Speakers and Titles:</strong></p>
                        <ul style="list-style: none; padding: 0; margin: 0; line-height: 1.6;">
                            <li>Prof. Dr. med. Katrin Amunts (30 Min.): <strong>EBRAINS ‚Äì concepts, services and
                                    applications</strong> <a href="https://www.fz-juelich.de/profile/amunts_k"
                                    target="_blank" style="text-decoration: underline;">profile</a></li>
                            <li>Prof. Trygve Leergaard (30 Min.): <strong>Brain Atlases</strong> <a
                                    href="https://www.med.uio.no/imb/english/people/aca/leergaar/" target="_blank"
                                    style="text-decoration: underline;">profile</a></li>
                            <li>Oliver Schmid (30 Min.): <strong>The EBRAINS Knowledge Graph - a scientific metadata
                                    management solution</strong> <a href="https://www.cscs.ch/about/staff"
                                    target="_blank" style="text-decoration: underline;">profile</a></li>
                        </ul>
                    </div>
                    <p class="abstract"><strong>Abstract:</strong> This seminar, as part of an ongoing series, will
                        introduce the EBRAINS-RI, featuring a presentation by Prof. Katrin Amunts, co-CEO of EBRAINS.
                        The session will also provide an overview of two key components of the infrastructure: the
                        EBRAINS Knowledge Graph (KG) and the EBRAINS Brain Atlases by Oliver Schmid and Trygve Leergaard
                        respectively. The EBRAINS Knowledge Graph is the metadata management system that underpins
                        EBRAINS' Data and Knowledge Services. It offers essential tools and services designed to make
                        neuroscientific data, models, and software FAIR (Findable, Accessible, Interoperable, and
                        Reusable). EBRAINS Brain Atlases provide comprehensive maps of brain regions defined based on
                        structure, function and neural connections. As spatial reference systems for neuroscience, they
                        are essential for understanding the complexity of the healthy brain, studying brain disorders
                        and seeking to develop new treatments.</p>
                    <div class="card-links">
                        <a href="https://youtu.be/5axlpdjX8FU" target="_blank" class="link-btn">üé• YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2024-2 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/11/12 Tue 18:00-19:30 (JST)</div>
                    <h3 class="card-title">Virtual Brain Twins in Medicine</h3>
                    <div class="speaker">
                        <div class="speaker-name"><a href="https://ins-amu.fr/jirsaviktor" target="_blank"
                                rel="noopener noreferrer" style="text-decoration: underline;">Viktor Jirsa</a></div>
                        <div class="speaker-affiliation">Institut de
                            Neurosciences des Syst√®mes</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> In the past twenty years, we have made significant
                        progress in creating digital models of an individual‚Äôs brain, so called virtual brain twins. By
                        combining brain imaging data with mathematical models, we can predict outcomes more accurately
                        than using each method separately. Our approach has helped us understand normal brain states,
                        their operation and conditions like healthy aging, dementia and epilepsy. Using a combination of
                        computational modeling and dynamical systems analysis we provide a mechanistic description of
                        the formation of resting state manifold via the network connectivity. We demonstrate that the
                        symmetry breaking by the connectivity creates a characteristic flow on the manifold, which
                        produces the major data features across scales and imaging modalities. These include spontaneous
                        high amplitude co-activations, neuronal cascades, spectral cortical gradients, multistability,
                        and characteristic functional connectivity dynamics. When aggregated across cortical
                        hierarchies, these match the profiles from empirical data and explain features of the brain‚Äôs
                        microstate organization. The digital brain twin augments the value of empirical data by
                        completing missing data, allowing clinical hypothesis testing and optimizing treatment
                        strategies for the individual patient. Virtual Brain Twins are part of the European
                        infrastructure called EBRAINS, which supports researchers worldwide in digital neuroscience.</p>
                    <div class="card-links">
                        <a href="https://youtu.be/4duX5dLS19Y?si=_fBGQVz_5clG6nrZ" target="_blank" class="link-btn">üé•
                            YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2024-3 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/10/22 Tue 13:00-14:00 (JST)</div>
                    <h3 class="card-title">Hands-on tutorial for OptiNiSt</h3>
                    <div class="speaker">
                        <div class="speaker-name">Yukako Yamane</div>
                        <div class="speaker-affiliation">OIST</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> OptiNiSt (<a href="https://optinist.readthedocs.io/"
                            target="_blank" style="text-decoration: underline;">https://optinist.readthedocs.io/</a>) is
                        an open source software tool that helps you to build calcium neuroimage data analysis pipelines
                        by comparing and combining multiple tools through graphical user interface and to produce data
                        processing scripts. In order to promote this tool, developed by the Brain/MINDS project for
                        reproducibility and standardization of neural data analysis, we will hold a hands-on tutorial
                        session. You can use your own note PC to experience how to build a data analysis pipeline. We
                        recommend downloading the Docker image of OptiNiSt according to the instructions (<a
                            href="https://github.com/oist/optinist_tutorial_preparation" target="_blank"
                            style="text-decoration: underline;">https://github.com/oist/optinist_tutorial_preparation</a>).
                    </p>
                    <div class="card-links">
                        <a href="https://youtu.be/5WmRXQirWMI" target="_blank" class="link-btn">üé• YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2024-4 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/9/30 Mon 13:00-14:00 (JST)</div>
                    <h3 class="card-title">Tutorial on NIfTI Files, 3D Slicer and Image Registration using ANTs</h3>
                    <div class="speaker">
                        <div class="speaker-name">Rui Gong</div>
                        <div class="speaker-affiliation">ExCELLS, NINS</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> In this tutorial, we will go over how to use 3D
                        Slicer to load and view NIfTI data, brain atlas and labels. We will be using one ex vivo mouse
                        brain data as an example to perform image registration to a reference template (Turone Mouse
                        Brain Template) space and then apply inverse transforms to the atlas to view it in individual
                        space.</p>
                    <div class="card-links">
                        <a href="https://youtu.be/c3TsjtgU0lU" target="_blank" class="link-btn">üé• YouTube</a>
                        <a href="https://www.notion.so/Digital-Brain-Seminar-90cc94badac64d32a281cba4245ed66d?pvs=21"
                            target="_blank" class="link-btn">üîó Registration Page</a>
                        <a href="https://www.notion.so/Digital-Brain-Tutorial-521cb473558c41d6b733555fe0b53e8b?pvs=21"
                            target="_blank" class="link-btn">üìö Tutorial Series</a>
                    </div>
                </div>
            </article>

            <!-- 2024-5 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/9/19-21</div>
                    <h3 class="card-title">1st Digital Brain Workshop</h3>
                    <div class="speaker">
                        <div class="speaker-name">Kyushu University Nihonbashi Satellite</div>
                        <div class="speaker-affiliation">on-site only</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Event:</strong> The first Digital Brain Workshop held as an on-site
                        event at Kyushu University Nihonbashi Satellite.</p>
                    <div class="card-links">
                        <a href="https://boatneck-weeder-7b7.notion.site/1st-Digital-Brain-Workshop-131a68936dda4867a88fedd25dfaac92"
                            target="_blank" class="link-btn">üîó Workshop Page</a>
                    </div>
                </div>
            </article>

            <!-- 2024-Tutorial-1 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/9/6 Fri 10:00-18:00 (JST)</div>
                    <h3 class="card-title">1st Digital Brain Tutorial: MRI Data Usage & Analysis</h3>
                    <div class="speaker">
                        <div class="speaker-name">International Brain Protocol</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Topic:</strong> Tutorial on MRI data usage and analysis for the
                        International Brain Protocol.<br><strong>Format:</strong> Hybrid (Registration necessary).</p>
                    <div class="card-links">
                        <a href="https://www.notion.so/Digital-Brain-Tutorial-521cb473558c41d6b733555fe0b53e8b?pvs=21"
                            target="_blank" class="link-btn">üîó Tutorial Page</a>
                    </div>
                </div>
            </article>

            <!-- 2024-6 (5th Seminar) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/7/4 Thu 13:30-17:00 (JST)</div>
                    <h3 class="card-title">5th Seminar: Data sharing and standardization in human neuroscience</h3>
                    <div class="speaker">
                        <div class="speaker-entry">
                            <div class="speaker-name">Franco Pestilli</div>
                            <div class="speaker-affiliation">University of Texas</div>
                        </div>
                        <div class="speaker-entry">
                            <div class="speaker-name">Jean-Baptiste Poline</div>
                            <div class="speaker-affiliation">McGill University</div>
                        </div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <div class="place-info" style="margin-bottom: 2rem; font-size: 0.95rem; color: #475569;">
                        <p><strong>Chair:</strong> Saori C Tanaka (ATR)</p>
                        <p><strong>Place:</strong> Hybrid: ATR + Zoom</p>
                        <p style="margin-top: 0.5rem;">Main conference room in B1F, ATR, Kyoto<br>
                            <a href="https://www.atr.jp/map_etc/access_e.html" target="_blank"
                                style="text-decoration: underline;">https://www.atr.jp/map_etc/access_e.html</a>
                        </p>
                    </div>

                    <div class="talk-section" style="margin-bottom: 2rem;">
                        <p style="margin-bottom: 0.5rem;"><strong>13:30 - 14:30: Franco Pestilli</strong> (University of
                            Texas)</p>
                        <p style="margin-bottom: 0.5rem; color: var(--accent);"><strong>Title: Putting brain data and
                                cloud technology to good use</strong></p>
                        <p class="abstract">Neuroscience research has expanded dramatically over the past 30 years by
                            advancing standardization and tool development to support rigor and transparency.
                            Consequently, the complexity of the data pipeline has also increased, hindering access to
                            FAIR (Findable, Accessible, Interoperabile, and Reusable) data analysis to portions of the
                            worldwide research community. I will present, <a href="http://brainlife.io/" target="_blank"
                                style="text-decoration: underline;">brainlife.io</a> the platform was developed to
                            reduce these burdens and democratize modern neuroscience research across institutions and
                            career levels. Using community software and hardware infrastructure, the platform provides
                            open-source data standardization, management, visualization, and processing and simplifies
                            the data pipeline. <a href="http://brainlife.io/" target="_blank"
                                style="text-decoration: underline;">brainlife.io</a> automatically tracks the provenance
                            history of thousands of data objects, supporting simplicity, efficiency, and transparency in
                            neuroscience research. Here this http <a href="http://brainlife.io/" target="_blank"
                                style="text-decoration: underline;">brainlife.io</a>'s technology and data services are
                            described and evaluated for validity, reliability, reproducibility, replicability, and
                            scientific utility. Using data from 4 modalities and 3,200 participants, we demonstrate that
                            this http <a href="http://brainlife.io/" target="_blank"
                                style="text-decoration: underline;">brainlife.io</a>'s services produce outputs that
                            adhere to best practices in modern neuroscience research.</p>
                    </div>

                    <div class="talk-section" style="margin-bottom: 2rem;">
                        <p style="margin-bottom: 0.5rem;"><strong>14:45 - 15:45: Jean-Baptiste Poline</strong> (McGill
                            University)</p>
                        <p style="margin-bottom: 0.5rem; color: var(--accent);"><strong>Title: Changing the landscape of
                                datasharing in brain research with standardized distributed infrastructures: a
                                Neurobagel journey</strong></p>
                        <p class="abstract">Data sharing in human neuroimaging remains a critical component of many
                            research projects, in particular when machine learning models for predicting diagnosis or
                            disease progression need to be fitted or tested on data from different cohorts. Data sharing
                            remains very hard for clinical researchers who i) don't always have the technical resources
                            ii) face ethical and legal barriers iii) have institutional incentives to keep the data
                            local. We propose a change in paradigm in neuroimaging datasharing. Current solutions are
                            often centralized and need complex data sharing research agreements and adaptation to legal
                            frameworks, for instance the GDPR in Europe. We present Neurobagel, a distributed data
                            sharing solution based on neuroimaging and clinical standards, that enable search of
                            participant data across the world. Our current implementation already has more than 8 nodes
                            and 30,000 participant data (healthy or patients).</p>
                    </div>

                    <div class="talk-section">
                        <p style="margin-bottom: 0.5rem;"><strong>16:00 - 17:00: Panel Discussion</strong></p>
                        <p><strong>Chair:</strong> Saori C Tanaka, ATR</p>
                    </div>
                </div>
            </article>

            <!-- 2024-7 (4th Seminar) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/6/13 Thr 12:00-13:00 (JST)</div>
                    <h3 class="card-title">4th Seminar: Creating neuromorphic artificial intelligence using reverse
                        engineering of generative models</h3>
                    <div class="speaker">
                        <div class="speaker-name">Takuya Isomura</div>
                        <div class="speaker-affiliation">RIKEN Center for Brain Science</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> Empirical applications of the free-energy principle
                        at the cellular and synaptic levels are not straightforward because they entail a commitment to
                        a particular process theory (i.e., neuronal basis). To address this issue, we developed a
                        reverse engineering technique that links quantities in neuronal networks to those in Bayesian
                        inference and showed that any canonical neural network‚Äîwhose activity and plasticity minimise a
                        shared Helmholtz energy‚Äîcan be cast as performing variational Bayesian inference. By combining
                        with an in vitro causal inference paradigm, we experimentally validated the free-energy
                        principle by showing its ability to predict the quantitative self-organisation of in vitro
                        neural networks. We have recently begun to apply this technique to neural activity of zebrafish
                        and rodents to reverse engineer their generative models. The virtues of the reverse engineering
                        are that, when provided with initial empirical data, it enables the systematic identification of
                        a generative model employed by the biological system. The reconstructed generative model yields
                        a neuromorphic artificial intelligence that performs Bayesian inference. This further enables
                        the quantitative predictions of subsequent self-organisation and learning in the system.</p>

                    <div style="margin-top: 1.5rem; text-align: center;">
                        <img src="images/isomura_seminar.png" alt="Reverse engineering of generative models diagram"
                            style="width: 100%; max-width: 800px; border-radius: 8px; box-shadow: 0 4px 20px rgba(0,0,0,0.15);">
                    </div>
                </div>
            </article>

            <!-- 2024-8 -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/5/10 Fri 13:00-14:30 (JST)</div>
                    <h3 class="card-title">Creating bridges between the digital and physical realms with 3D vision</h3>
                    <div class="speaker">
                        <div class="speaker-name">Thomas Diego</div>
                        <div class="speaker-affiliation">Kyushu University</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract"><strong>Abstract:</strong> Our modern societies strongly rely on machines to
                        survive. These machines treat information in the digital world but interact with humans in the
                        physical world. To bridge the gap between the digital and physical realms, we have dedicated our
                        efforts to the creation of new AI-based 3D vision models. Our research has been centered on the
                        task of capturing and modeling the human body, as this is fundamental for enhancing
                        human-machine interactions. Leveraging generative AI, which learns from vast collections of
                        images and videos, we have been able to gain insights into human body shapes, deformations, and
                        semantic interactions within various scenes. This research represents a significant step toward
                        the development of Large 3D Vision Models, which are essential for advancing machines toward
                        greater autonomy and intelligence. In this talk, I will present the latest innovations in the
                        creation of digital and autonomous human avatars, showcasing how these advancements are shaping
                        the future of human-machine interactions.</p>
                    <div class="card-links">
                        <a href="https://youtu.be/3Z7SIKhO26M" target="_blank" class="link-btn">üé•
                            YouTube</a>
                    </div>
                </div>
            </article>

            <!-- 2024-9 (2nd Seminar) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/4/22 Mon 13:00-15:40 (JST)</div>
                    <h3 class="card-title">2nd Seminar: Multiple Talks Session</h3>
                    <div class="speaker">
                        <div class="speaker-name">Ken Nakae, Hiromichi Tsukada, Hiroshi Ishii, Keiichi Ueda, Yoshitaro
                            Tanaka</div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <p class="abstract" style="margin-bottom: 1.5rem;"><strong>Place:</strong> Zoom</p>

                    <div class="schedule-section">
                        <div class="talk-item"
                            style="margin-bottom: 1.5rem; padding-bottom: 1rem; border-bottom: 1px solid #f1f5f9;">
                            <p style="margin-bottom: 0.3rem;"><strong>13:00-13:30 Ken Nakae</strong> (ExCELLS, NINS)</p>
                            <p style="color: var(--accent); font-weight: 600; margin-bottom: 0.5rem;">How to use the
                                Brian/MINDS data portal.</p>
                            <a href="https://www.dropbox.com/scl/fi/yci2ia0l70uoq35db46f9/DBSv2.pdf?rlkey=v9vk1mg3v7vmh56o2bz9ma1li&dl=0"
                                target="_blank"
                                style="font-size: 0.85rem; color: #64748b; text-decoration: underline;">üìÑ Presentation
                                Slide</a>
                        </div>

                        <div class="talk-item"
                            style="margin-bottom: 1.5rem; padding-bottom: 1rem; border-bottom: 1px solid #f1f5f9;">
                            <p style="margin-bottom: 0.3rem;"><strong>13:30-14:00 Hiromichi Tsukada</strong> (CMSAI,
                                Chubu Univ)</p>
                            <p style="color: var(--accent); font-weight: 600;">Connectome-based modeling using marmoset
                                MRI and gene expression data</p>
                        </div>

                        <div class="talk-item"
                            style="margin-bottom: 1.5rem; padding-bottom: 1rem; border-bottom: 1px solid #f1f5f9;">
                            <p style="margin-bottom: 0.3rem;"><strong>14:10-14:40 Hiroshi Ishii</strong> (Research
                                Institute for Electronic Science, Hokkaido University)</p>
                            <p style="color: var(--accent); font-weight: 600;">Pattern formation in mathematical models
                                including neuronal interaction effects</p>
                        </div>

                        <div class="talk-item"
                            style="margin-bottom: 1.5rem; padding-bottom: 1rem; border-bottom: 1px solid #f1f5f9;">
                            <p style="margin-bottom: 0.3rem;"><strong>14:40-15:10 Keiichi Ueda</strong> (Faculty of
                                Science, Academic Assembly, University of Toyama)</p>
                            <p style="color: var(--accent); font-weight: 600;">Decentralized distributed parameter
                                tuning model for coupled oscillator systems</p>
                        </div>

                        <div class="talk-item" style="margin-bottom: 1.5rem;">
                            <p style="margin-bottom: 0.3rem;"><strong>15:10-15:40 Yoshitaro Tanaka</strong> (School of
                                Systems Information Science, Future University Hakodate)</p>
                            <p style="color: var(--accent); font-weight: 600;">Proposal of a mathematical model of a
                                reservoir computing using the diffusive chemical reaction</p>
                        </div>
                    </div>

                    <div class="card-links"
                        style="margin-top: 1.5rem; padding-top: 1rem; border-top: 2px solid #e2e8f0;">
                        <a href="https://www.youtube.com/watch?v=83YRBKRUdxk" target="_blank" class="link-btn">üé•
                            YouTube (Ken Nakae)</a>
                    </div>
                </div>
            </article>

            <!-- 2024-10 (1st Seminar) -->
            <article class="archive-card collapsible">
                <div class="card-header">
                    <div class="badge">2024/4/1 Mon 15:00-17:30 (JST)</div>
                    <h3 class="card-title">1st Seminar: Learning of hidden principled structures behind observations /
                        What is the Digital Brain</h3>
                    <div class="speaker">
                        <div class="speaker-entry">
                            <div class="speaker-name"><a href="https://takerum.github.io/" target="_blank"
                                    rel="noopener noreferrer" style="text-decoration: underline;">Takeru Miyato</a>
                            </div>
                            <div class="speaker-affiliation">University of T√ºbingen</div>
                        </div>
                        <div class="speaker-entry">
                            <div class="speaker-name">Kenji Doya</div>
                            <div class="speaker-affiliation">OIST</div>
                        </div>
                    </div>
                    <span class="expand-icon">‚ñº</span>
                </div>
                <div class="card-content">
                    <!-- 1st Speaker -->
                    <div class="talk-section" style="margin-bottom: 2.5rem;">
                        <p style="margin-bottom: 0.5rem; color: #64748b; font-size: 0.9rem;">15:00 - 16:30</p>
                        <p style="margin-bottom: 0.5rem;"><strong>1st Speaker:</strong> <a
                                href="https://takerum.github.io/" target="_blank"
                                style="text-decoration: underline;">Takeru Miyato</a> (University of T√ºbingen)</p>

                        <div class="place-info"
                            style="margin-bottom: 1rem; padding: 0.8rem; background-color: #f8fafc; border-radius: 6px; font-size: 0.9rem; color: #475569;">
                            <p><strong>Place:</strong> Hybrid: Kyoto University + Zoom</p>
                            <p>328, South, Research Bldg. No 8, Kyoto University</p>
                            <a href="https://www.kyoto-u.ac.jp/en/access/main-campus-map" target="_blank"
                                style="text-decoration: underline;">Campus Map</a>
                        </div>

                        <p style="margin-bottom: 0.5rem; color: var(--accent); font-weight: 600;">Title: Learning of
                            hidden principled structures behind observations</p>
                        <p class="abstract"><strong>Abstract:</strong> Recent advancements in AI have utilized extensive
                            data sets to train models capable of understanding and generating observations across
                            various modalities. Notably, Large Language Models (LLMs) based on transformer architectures
                            excel in natural language processing, often outperforming the average human. This success
                            demonstrates AI's ability to grasp the complex structures underlying human language.
                            However, a significant challenge remains: AI models do not possess a clear method to
                            communicate the "generalizable structures"‚Äîthe underlying patterns and principles‚Äîthey
                            learned from data. Unlike humans, who can innovate language through the creation of new
                            terms and grammatical rules‚Äîas exemplified by the development of mathematical languages to
                            elegantly describe physical phenomena, AI lacks a mechanism to explicitly reveal the
                            structures and principles they have learned. The core issue stems from AI's inability to
                            "invent" their own languages or to recognize and express the implicit structures acquired
                            through training. By developing AI systems capable of not only learning from data but also
                            articulating and refining the implicitly learned structures, we would mark a significant
                            leap towards machines that can think, learn, and create in ways more akin to human
                            cognition. In this talk, I will present our recent work aiming at enabling machines to
                            explicitly learn the hidden principled mechanisms behind real-world observations, such as
                            disentangled and equivariant structures. Additionally, I will discuss an ongoing project
                            focusing on neural synchrony-based object discovery, inspired by the temporal coding
                            hypothesis in neuroscience.</p>
                    </div>

                    <!-- 2nd Speaker -->
                    <div class="talk-section" style="border-top: 1px solid #e2e8f0; padding-top: 2rem;">
                        <p style="margin-bottom: 0.5rem; color: #64748b; font-size: 0.9rem;">16:30 - 17:30</p>
                        <p style="margin-bottom: 0.5rem;"><strong>2nd Speaker:</strong> Kenji Doya (Okinawa Institute of
                            Science and Technology Graduate University)</p>
                        <p style="margin-bottom: 1rem; font-size: 0.9rem; color: #64748b;">ÈäÖË∞∑ Ë≥¢Ê≤ªÔºàÊ≤ñÁ∏ÑÁßëÂ≠¶ÊäÄË°ìÂ§ßÂ≠¶Èô¢Â§ßÂ≠¶ Á•ûÁµåË®àÁÆó„É¶„Éã„ÉÉ„ÉàÔºâ
                        </p>

                        <div class="place-info"
                            style="margin-bottom: 1rem; padding: 0.8rem; background-color: #f8fafc; border-radius: 6px; font-size: 0.9rem; color: #475569;">
                            <p><strong>Place:</strong> Same (Speaker is online)</p>
                        </div>

                        <p style="margin-bottom: 0.3rem; color: var(--accent); font-weight: 600;">Title: What is the
                            Digital Brain of Brain/MINDS 2.0</p>
                        <p style="margin-bottom: 1rem; font-size: 0.95rem; color: var(--accent);">
                            ËÑ≥Á•ûÁµåÁßëÂ≠¶Áµ±Âêà„Éó„É≠„Ç∞„É©„É†„Äå„Éá„Ç∏„Çø„É´ËÑ≥„Äç„ÅÆ„ÇÅ„Åñ„Åô„ÇÇ„ÅÆ</p>

                        <p class="abstract"><strong>Abstract:</strong> Following Japan's Brain/MINDS project since 2014,
                            the next project, nicknamed Brain/MINDS 2.0, is starting this year. A remarkable feature of
                            this new project is that the "digital brain‚Äù plays a central role in integrating brain data
                            at multiple scales from multiple species and for understanding the brain functions and
                            neuropsychiatric disorders. But what exactly is the digital brain? As project‚Äôs core
                            organization member in charge of the digital brain development, I will present what is the
                            current design of the digital brain, what kind of data, technologies, and infrastructures
                            are require, and what outcomes are expected. A call for specific research proposals is now
                            open till April 10th (<a href="https://www.amed.go.jp/koubo/15/01/1501B_00104.html"
                                target="_blank" style="text-decoration: underline;">AMED Call</a>). We hope this talk
                            will help you plan your proposal to be best connected to the digital brain.</p>
                    </div>

                    <div class="card-links" style="margin-top: 2rem; padding-top: 1rem; border-top: 2px solid #e2e8f0;">
                        <a href="https://www.dropbox.com/scl/fi/x0eeqy623p8sx3lvqv6v3/Doya2024DigitalBrain.pdf?rlkey=t1eb3b90fw2zp6pann5yn5688&dl=0"
                            target="_blank" class="link-btn">üìÑ Slide (Doya)</a>
                        <a href="https://www.youtube.com/watch?v=hDqiOSKc8Ks" target="_blank" class="link-btn">üé•
                            YouTube (Doya)</a>
                    </div>
                </div>
            </article>
        </section>
    </div>

    <footer>
        <div class="footer-grid">
            <div class="footer-col">
                <h3>Organizers</h3>
                <ul>
                    <li><strong><a href="https://researchmap.jp/ken.nakae" target="_blank" rel="noopener noreferrer"
                                style="color: inherit; text-decoration: underline;">Ken Nakae</a></strong> (Fukui
                        University)</li>
                    <li><strong><a href="https://researchmap.jp/206s16" target="_blank" rel="noopener noreferrer"
                                style="color: inherit; text-decoration: underline;">Daisuke Tagami</a></strong> (Kyushu
                        University)</li>
                    <li><strong><a href="https://researchmap.jp/doya" target="_blank" rel="noopener noreferrer"
                                style="color: inherit; text-decoration: underline;">Kenji Doya</a></strong> (OIST)</li>
                </ul>
            </div>

            <div class="footer-col">
                <h3>Sponsors</h3>
                <ul>
                    <li><a href="https://www.amed.go.jp/en/" target="_blank" rel="noopener noreferrer"
                            style="color: inherit; text-decoration: underline;">Japan Agency for Medical Research and
                            Development (AMED)</a></li>
                    <li><a href="https://mfip.jp/" target="_blank" rel="noopener noreferrer"
                            style="color: inherit; text-decoration: underline;">Mathematics for Industry Platform
                            (MfIP)</a></li>
                    <li><a href="https://braidyn-bc.jp/" target="_blank" rel="noopener noreferrer"
                            style="color: inherit; text-decoration: underline;">Grant-in-Aid for Transformative Research
                            Areas, MEXT</a></li>
                </ul>
            </div>

            <div class="footer-col">
                <h3>Supported by</h3>
                <ul>
                    <li><a href="https://www.internationalbraininitiative.org/" target="_blank"
                            rel="noopener noreferrer" style="color: inherit; text-decoration: underline;">International
                            Brain Initiative (IBI)</a></li>
                    <li><a href="https://www.brainscience-union.jp/" target="_blank" rel="noopener noreferrer"
                            style="color: inherit; text-decoration: underline;">Union of Brain Science Associations in
                            Japan</a></li>
                    <li><a href="https://jnns.org/" target="_blank" rel="noopener noreferrer"
                            style="color: inherit; text-decoration: underline;">Japanese Neural Network Society</a></li>
                </ul>
            </div>
        </div>

        <div class="copyright">
            &copy; Digital Brain Project
        </div>
    </footer>

    <script>
        const hamburgerBtn = document.getElementById('hamburger-btn');
        const navLinks = document.getElementById('nav-links');

        hamburgerBtn.addEventListener('click', () => {
            navLinks.classList.toggle('active');
        });

        // Year tab navigation
        document.querySelectorAll('.year-tab').forEach(tab => {
            tab.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelectorAll('.year-tab').forEach(t => t.classList.remove('active'));
                this.classList.add('active');
                const targetId = this.getAttribute('href').substring(1);
                document.querySelectorAll('.year-section').forEach(section => {
                    section.style.display = section.id === targetId ? 'block' : 'none';
                });
            });
        });

        // Collapsible cards
        document.querySelectorAll('.archive-card.collapsible .card-header').forEach(header => {
            header.addEventListener('click', function (e) {
                // Don't toggle if clicking on a link
                if (e.target.tagName === 'A') return;

                const card = this.closest('.archive-card');
                card.classList.toggle('expanded');
            });
        });

        // Initialize: show 2025, hide 2024
        document.getElementById('year-2024').style.display = 'none';
    </script>

    <!-- Three.js Neural Network Animation -->
    <script>
        (function () {
            const canvas = document.getElementById('hero-canvas');
            const heroSection = document.querySelector('.archive-hero');

            if (!canvas || !heroSection) return;

            // Scene setup
            const scene = new THREE.Scene();
            const camera = new THREE.PerspectiveCamera(75, heroSection.offsetWidth / heroSection.offsetHeight, 0.1, 1000);
            const renderer = new THREE.WebGLRenderer({ canvas, alpha: true, antialias: true });

            renderer.setSize(heroSection.offsetWidth, heroSection.offsetHeight);
            renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));

            // Particles
            const particleCount = 80;
            const particles = [];
            const particleGeometry = new THREE.SphereGeometry(0.08, 8, 8);
            const particleMaterial = new THREE.MeshBasicMaterial({ color: 0xffffff, transparent: true, opacity: 0.7 });

            for (let i = 0; i < particleCount; i++) {
                const particle = new THREE.Mesh(particleGeometry, particleMaterial);
                particle.position.x = (Math.random() - 0.5) * 15;
                particle.position.y = (Math.random() - 0.5) * 8;
                particle.position.z = (Math.random() - 0.5) * 8;
                particle.velocity = {
                    x: (Math.random() - 0.5) * 0.01,
                    y: (Math.random() - 0.5) * 0.01,
                    z: (Math.random() - 0.5) * 0.01
                };
                particles.push(particle);
                scene.add(particle);
            }

            // Lines connecting particles
            const lineMaterial = new THREE.LineBasicMaterial({ color: 0xffffff, transparent: true, opacity: 0.15 });
            let linesMesh = null;

            function updateLines() {
                if (linesMesh) scene.remove(linesMesh);

                const positions = [];
                const maxDistance = 3;

                for (let i = 0; i < particles.length; i++) {
                    for (let j = i + 1; j < particles.length; j++) {
                        const distance = particles[i].position.distanceTo(particles[j].position);
                        if (distance < maxDistance) {
                            positions.push(particles[i].position.x, particles[i].position.y, particles[i].position.z);
                            positions.push(particles[j].position.x, particles[j].position.y, particles[j].position.z);
                        }
                    }
                }

                const geometry = new THREE.BufferGeometry();
                geometry.setAttribute('position', new THREE.Float32BufferAttribute(positions, 3));
                linesMesh = new THREE.LineSegments(geometry, lineMaterial);
                scene.add(linesMesh);
            }

            camera.position.z = 8;

            // Mouse interaction
            let mouseX = 0, mouseY = 0;
            heroSection.addEventListener('mousemove', (e) => {
                const rect = heroSection.getBoundingClientRect();
                mouseX = ((e.clientX - rect.left) / rect.width - 0.5) * 2;
                mouseY = -((e.clientY - rect.top) / rect.height - 0.5) * 2;
            });

            // Animation loop
            function animate() {
                requestAnimationFrame(animate);

                // Move particles
                particles.forEach(particle => {
                    particle.position.x += particle.velocity.x;
                    particle.position.y += particle.velocity.y;
                    particle.position.z += particle.velocity.z;

                    // Boundary check
                    if (Math.abs(particle.position.x) > 7.5) particle.velocity.x *= -1;
                    if (Math.abs(particle.position.y) > 4) particle.velocity.y *= -1;
                    if (Math.abs(particle.position.z) > 4) particle.velocity.z *= -1;
                });

                // Update lines every few frames for performance
                if (Math.random() > 0.9) updateLines();

                // Camera follows mouse slightly
                camera.position.x += (mouseX * 0.5 - camera.position.x) * 0.02;
                camera.position.y += (mouseY * 0.3 - camera.position.y) * 0.02;
                camera.lookAt(0, 0, 0);

                renderer.render(scene, camera);
            }

            animate();
            updateLines();

            // Handle resize
            window.addEventListener('resize', () => {
                camera.aspect = heroSection.offsetWidth / heroSection.offsetHeight;
                camera.updateProjectionMatrix();
                renderer.setSize(heroSection.offsetWidth, heroSection.offsetHeight);
            });
        })();
    </script>

</body>

</html>